{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2daa78e8-e4e8-4ce9-ae99-073277051e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16659 tasks with task similarity > 0 and title similarity > 0\n",
      "1851 startups out of 2927 decomposed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from llms import gemini\n",
    "from llms import chatGPT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "   \n",
    "def printExamples():\n",
    "    grouped = df.groupby(['organization name', 'website'])\n",
    "    for (org_name, website), group_data in grouped:\n",
    "        print(\"***************************\")\n",
    "        print(f\"Organization: {org_name}, Website: {website}\")\n",
    "        for col in ['generated_description', 'Tasks/Jobs', 'Industry', 'Customers', 'generated_description_conf_interval', 'parsed_description_conf_interval']:\n",
    "            print(f\"{col}: {group_data.iloc[0][col]}\")\n",
    "        for example, example_data in group_data.groupby('example'):\n",
    "            print(\"_________________________\")\n",
    "            print(f\"Example: {example}\")\n",
    "            for col in ['situation_conf_interval', 'situation_conf_interval_reasoning']:\n",
    "                print(f\"{col}: {example_data.iloc[0][col]}\")\n",
    "            for idx, row in example_data.iterrows():\n",
    "                print(\"##########################\")\n",
    "                for col in ['job', 'onet_title', 'onet_task', 'example_job_title', 'task_similarity', 'job_title_similarity', 'onet_weight']:\n",
    "                    print(f\"{col}: {row[col]}\")\n",
    "        print(\"***************************\\n\")\n",
    "        \n",
    "def writeExamples():\n",
    "    output_file_path = \"../output/examples.txt\"\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        grouped = df.groupby(['organization name', 'website'])\n",
    "        for (org_name, website), group_data in grouped:\n",
    "            f.write(\"***************************\\n\")\n",
    "            f.write(f\"Organization: {org_name}, Website: {website}\\n\")\n",
    "            for col in ['generated_description', 'Tasks/Jobs', 'Industry', 'Customers', 'generated_description_conf_interval', 'parsed_description_conf_interval']:\n",
    "                f.write(f\"{col}: {group_data.iloc[0][col]}\\n\")\n",
    "            for example, example_data in group_data.groupby('example'):\n",
    "                f.write(\"_________________________\\n\")\n",
    "                f.write(f\"Example: {example}\\n\")\n",
    "                for col in ['situation_conf_interval', 'situation_conf_interval_reasoning']:\n",
    "                    f.write(f\"{col}: {example_data.iloc[0][col]}\\n\")\n",
    "                for idx, row in example_data.iterrows():\n",
    "                    f.write(\"##########################\\n\")\n",
    "                    for col in ['job', 'onet_title', 'onet_task', 'example_job_title', 'task_similarity', 'job_title_similarity','onet_weight']:\n",
    "                        f.write(f\"{col}: {row[col]}\\n\")\n",
    "            f.write(\"***************************\\n\\n\")\n",
    "                \n",
    "def generateOutput(df,type):\n",
    "    conf_interval_text = 'all' #conf_interval if conf_interval != True else 'all'\n",
    "    output_file_path = f\"../output/output_{type}_{task_sim}_{title_sim}_{conf_interval_text}.txt\"\n",
    "    num_startups = len(df[\"organization name\"].unique())\n",
    "    num_onet_startups = len(df[\"organization_name\"].unique())\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        group_sums = df.groupby('Minor Group Name')['onet_weight'].sum().sort_values(ascending=False)\n",
    "        f.write(f\"{num_onet_startups} Startups Founded {type} launch of chatGPT with 1-10 employees\\n\")\n",
    "        \n",
    "        f.write(f\"Task Similarity: {task_sim}, Title Similarity {title_sim}, Confidence Interval: {conf_interval_text}\\n\")\n",
    "        f.write(f\"{num_onet_startups} startups out of {num_startups} decomposed\\n\")\n",
    "        f.write(\"Decomposition of startup effect on labor market\\n\")\n",
    "        \n",
    "        for group_name in group_sums.index:\n",
    "            group_data = df[df['Minor Group Name'] == group_name]\n",
    "            f.write(\"*****************\\n\")\n",
    "            f.write(f\"Group: {group_name}\\n\")\n",
    "            onet_titles = group_data.groupby('onet_title')['onet_weight'].sum().round(2).sort_values(ascending=False).reset_index()\n",
    "            onet_tasks = group_data.groupby('onet_task')['onet_weight'].sum().round(2).sort_values(ascending=False).reset_index()\n",
    "            \n",
    "            f.write(\"________________\\n\")\n",
    "            f.write(\"10 Most Highly Weighted Titles:\\n\")\n",
    "            for index, row in onet_titles.iterrows():\n",
    "                f.write(f\"Title: {row['onet_title']}, Weight: {row['onet_weight']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"10 Most Highly Weighted Tasks:\\n\")\n",
    "            for index, row in list(onet_tasks.iterrows())[:10]:\n",
    "                f.write(f\"Task: {row['onet_task']}, Weight: {row['onet_weight']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"10 Example Startups:\\n\")\n",
    "            startups = group_data.groupby(['organization_name','website'])['onet_weight'].sum().round(2).sort_values(ascending=False).reset_index()\n",
    "            for index, row in list(startups.iterrows())[:10]:\n",
    "                f.write(f\"Startup: {row['organization_name']}, Website: {row['website']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "def get_ONET(task_sim, title_sim, conf_interval=True):\n",
    "    onet_df = pd.read_csv(\"../output/onet_df.csv\")\n",
    "    onet_df = onet_df[(onet_df.task_similarity > task_sim) & (onet_df.job_title_similarity > title_sim)]\n",
    "    onet_df = onet_df[onet_df.situation_conf_interval > conf_interval].reset_index(drop=True)\n",
    "    onet_df = compute_ONET_weights(onet_df)\n",
    "    print(f\"{len(onet_df)} tasks with task similarity > {task_sim} and title similarity > {title_sim}\")\n",
    "    return onet_df\n",
    "\n",
    "def compute_ONET_weights(onet_df):\n",
    "    onet_weights = onet_df.groupby(\"organization_name\")[\"onet_title\"].count().apply(lambda x: 1/x).reset_index().rename({\"onet_title\": \"onet_weight\"}, axis=1)\n",
    "    onet_df = onet_df.merge(onet_weights, on=\"organization_name\")\n",
    "    return onet_df\n",
    "\n",
    "\n",
    "def get_startup_generated_LLM_titles(onet_df):\n",
    "    example_job_titles_df = onet_df.example_job_title.value_counts().apply(lambda x: x/3).reset_index()\n",
    "    return example_job_titles_df\n",
    "\n",
    "def print_top_100_ONET_tasks(onet_df):\n",
    "    top_100_ONETtasks = onet_df['onet_task'].value_counts().reset_index().head(100)\n",
    "    top_100_tasks.columns = ['Task', 'Frequency']\n",
    "    for index, row in top_100_tasks.iterrows():\n",
    "        print(f\"Task = {row['Task']}, Frequency = {row['Frequency']}\\n\")\n",
    "\n",
    "def get_startup_data():\n",
    "    startup_df = pd.read_csv(\"../output/df_with_examples.csv\")\n",
    "    cols = ['organization name', 'founded date', 'website', 'description_all', 'industries_parsed', 'generated_description', 'parsed_description', 'Tasks/Jobs', 'Industry', 'Customers', 'generated_description_conf_interval', 'parsed_description_conf_interval']\n",
    "    df = startup_df[cols]\n",
    "    df = df.merge(onet_df,left_on=\"organization name\",right_on=\"organization_name\")\n",
    "\n",
    "\n",
    "    onet_occ = pd.read_csv(\"../input/onet_2023/Occupation Data.csv\")[[\"O*NET-SOC Code\",\"Title\"]]\n",
    "    onet_occ.columns = [\"Detailed Occupation\",\"onet_title\"]\n",
    "    df = df.merge(onet_occ,on=\"onet_title\")\n",
    "    df[\"Detailed Occupation\"] = df[\"Detailed Occupation\"].apply(lambda x: x[:-3])\n",
    "    codes = pd.read_csv(\"../input/soc_codes/soc_codes.csv\", index_col=0)\n",
    "    df = df.merge(codes,on=\"Detailed Occupation\", how=\"left\")\n",
    "\n",
    "    \n",
    "    num_startups = len(startup_df[\"organization name\"].unique())\n",
    "    num_onet_startups = len(df[\"organization_name\"].unique())\n",
    "    print(f\"{num_onet_startups} startups out of {num_startups} decomposed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_example_task_embeddings(df):\n",
    "    embedding_df = pd.read_csv('../output/df_with_examples_embeddings.csv')\n",
    "    melted_examples = pd.melt(embedding_df, id_vars=['organization name'], value_vars=['Example1', 'Example2', 'Example3'], \n",
    "                        var_name='number', value_name='Example')\n",
    "    \n",
    "    melted_jobs = pd.melt(embedding_df, id_vars=['organization name'], value_vars=['Job1_embedding', 'Job2_embedding', 'Job3_embedding'], \n",
    "                          var_name='number', value_name='example_task_embedding')\n",
    "    melted_jobs.number = melted_jobs.number.apply(lambda x: x[3])\n",
    "    melted_examples.number = melted_examples.number.apply(lambda x: x[-1])\n",
    "    embedding_df = pd.merge(melted_examples, melted_jobs, on=['organization name', 'number'])\n",
    "    df = embedding_df.merge(df,left_on=[\"organization name\",\"Example\"],right_on=[\"organization name\",\"example\"])\n",
    "    return df \n",
    "\n",
    "\n",
    "def get_onet_task_embeddings(df):\n",
    "    df_exp = pd.read_csv('../input/gpts_labels/gpt_exposure_embeddings.csv')\n",
    "    df_exp = df_exp[[x for x in df_exp.columns if \"Unnamed\" not in x]]\n",
    "    df_exp = df_exp[[\"Task\",\"task_embedding\",\"title_embedding\",\"Task ID\"]]\n",
    "    df_exp = df_exp.rename({\"task_embedding\":\"onet_task_embedding\",\"title_embedding\":\"onet_title_embedding\"},axis=1)\n",
    "    df = df.merge(df_exp,left_on=\"onet_task\",right_on=\"Task\",how=\"left\")\n",
    "   \n",
    "    return df\n",
    "    \n",
    "def tsne():\n",
    "    df.example_task_embedding = df.example_task_embedding.apply(lambda x: [float(y) for y in x.strip(\"[]\").split(\", \")])\n",
    "    df.onet_task_embedding = df.onet_task_embedding.apply(lambda x: [float(y) for y in x.strip(\"[]\").split(\", \")])\n",
    "    df.onet_title_embedding = df.onet_title_embedding.apply(lambda x: [float(y) for y in x.strip(\"[]\").split(\", \")])\n",
    "    embeddings = np.vstack(df['example_task_embedding'].values)\n",
    "    tsne = TSNE(n_components=3, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df[[\"example_task_embedding_tsne1\",\"example_task_embedding_tsne2\"]] = tsne_results[:, [0,1]]\n",
    "    \n",
    "    \n",
    "    embeddings = np.vstack(df['onet_task_embedding'].values)\n",
    "    tsne = TSNE(n_components=3, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df[[\"onet_task_embedding_tsne1\",\"onet_task_embedding_tsne2\"]] = tsne_results[:, [0,1]]\n",
    "    \n",
    "    embeddings = np.vstack(df['onet_title_embedding'].values)\n",
    "    tsne = TSNE(n_components=3, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df[[\"onet_title_embedding_tsne1\",\"onet_title_embedding_tsne2\"]] = tsne_results[:, [0,1]]\n",
    "    \n",
    "    tsne = df[['example_task_embedding_tsne1',\n",
    "     'example_task_embedding_tsne2',\n",
    "     'onet_task_embedding_tsne1',\n",
    "     'onet_task_embedding_tsne2',\n",
    "     'onet_title_embedding_tsne1',\n",
    "     'onet_title_embedding_tsne2']]\n",
    "    tsne.to_csv(\"../output/results/tsne.csv\")\n",
    "    return tsne, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "task_sim=0\n",
    "title_sim=0\n",
    "onet_df = get_ONET(task_sim=task_sim, title_sim=title_sim)\n",
    "df = get_startup_data()\n",
    "writeExamples()\n",
    "pre = df[pd.to_datetime(df['founded date'])<='11/30/2022']\n",
    "post = df[pd.to_datetime(df['founded date'])>='11/30/2022']\n",
    "generateOutput(pre,\"pre\")\n",
    "generateOutput(post,\"post\")\n",
    "# df = get_example_task_embeddings(df)\n",
    "# df = get_onet_task_embeddings(df)\n",
    "# df.head()\n",
    "# df.to_csv(\"../output/bls_df.csv\")\n",
    "\n",
    "# tsne, df = tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "676d98cd-138d-461e-b734-696c354e70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre 3207, post 1087\n"
     ]
    }
   ],
   "source": [
    "pre = df[pd.to_datetime(df['founded date'])<='11/30/2022']\n",
    "post = df[pd.to_datetime(df['founded date'])>='11/30/2022']\n",
    "print(f\"pre {len(pre)}, post {len(post)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a443fe4f-d754-4218-975a-52ff6796885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre[\"organization name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbaead8c-eeb6-45f8-93b5-55523eddfea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post[\"organization name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a44cd7f8-549f-4b65-b95f-9665149ffb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre 2232, post 695\n"
     ]
    }
   ],
   "source": [
    "startup_df = pd.read_csv(\"../output/df_with_examples.csv\")\n",
    "pre = startup_df[pd.to_datetime(startup_df['founded date'])<='11/30/2022']\n",
    "post = startup_df[pd.to_datetime(startup_df['founded date'])>='11/30/2022']\n",
    "print(f\"pre {len(pre)}, post {len(post)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54ee2530-2a9e-4a0f-a37e-39d70c6dd28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6084229390681004"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1358/2232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34c7488f-4398-48f7-8dac-9aa9c8180019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6517985611510791"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "453/695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "003274ac-0031-477d-990b-cf0e69a7fd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x17464cad0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"founded date\")["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63c47051-b298-4c7a-aeeb-dbab1625d379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater New York Area, East Coast, Northeastern US\n",
      "San Francisco Bay Area, Silicon Valley, West Coast\n",
      "www.ultipa.com\n",
      "www.ultipa.com\n",
      "www.OLY.AI\n",
      "fuzz.land\n",
      "Greater Miami Area, East Coast, Southern US\n",
      "San Francisco Bay Area, Silicon Valley, West Coast\n",
      "San Francisco Bay Area, Silicon Valley, West Coast\n",
      "San Francisco Bay Area, Silicon Valley, West Coast\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(pre.website.iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4126d021-8b99-4fee-95cf-4cb3d6522705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>organization name</th>\n",
       "      <th>founded date</th>\n",
       "      <th>website</th>\n",
       "      <th>description_all</th>\n",
       "      <th>industries_parsed</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>parsed_description</th>\n",
       "      <th>Tasks/Jobs</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Customers</th>\n",
       "      <th>...</th>\n",
       "      <th>job_title_similarity</th>\n",
       "      <th>onet_weight</th>\n",
       "      <th>Detailed Occupation</th>\n",
       "      <th>Major Group</th>\n",
       "      <th>Minor Group</th>\n",
       "      <th>Broad Group</th>\n",
       "      <th>Title</th>\n",
       "      <th>Major Group Name</th>\n",
       "      <th>Minor Group Name</th>\n",
       "      <th>Broad Group Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FuzzLand</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>fuzz.land</td>\n",
       "      <td>FuzzLand is a leveraging AI tool that allows b...</td>\n",
       "      <td>['Web Development']</td>\n",
       "      <td>FuzzLand is an AI-powered platform that simpli...</td>\n",
       "      <td>Tasks/Jobs: Smart contract analysis, Vulnerabi...</td>\n",
       "      <td>Smart contract analysis, Vulnerability detecti...</td>\n",
       "      <td>Blockchain security</td>\n",
       "      <td>Smart contract developers, Auditors, Blockchai...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694171</td>\n",
       "      <td>0.25</td>\n",
       "      <td>11-3031</td>\n",
       "      <td>11-0000</td>\n",
       "      <td>11-3000</td>\n",
       "      <td>11-3030</td>\n",
       "      <td>Financial Managers</td>\n",
       "      <td>Management Occupations</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "      <td>Financial Managers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  organization name founded date    website  \\\n",
       "9          FuzzLand   2022-01-01  fuzz.land   \n",
       "\n",
       "                                     description_all    industries_parsed  \\\n",
       "9  FuzzLand is a leveraging AI tool that allows b...  ['Web Development']   \n",
       "\n",
       "                               generated_description  \\\n",
       "9  FuzzLand is an AI-powered platform that simpli...   \n",
       "\n",
       "                                  parsed_description  \\\n",
       "9  Tasks/Jobs: Smart contract analysis, Vulnerabi...   \n",
       "\n",
       "                                          Tasks/Jobs             Industry  \\\n",
       "9  Smart contract analysis, Vulnerability detecti...  Blockchain security   \n",
       "\n",
       "                                           Customers  ...  \\\n",
       "9  Smart contract developers, Auditors, Blockchai...  ...   \n",
       "\n",
       "   job_title_similarity  onet_weight Detailed Occupation Major Group  \\\n",
       "9              0.694171         0.25             11-3031     11-0000   \n",
       "\n",
       "   Minor Group Broad Group               Title        Major Group Name  \\\n",
       "9      11-3000     11-3030  Financial Managers  Management Occupations   \n",
       "\n",
       "                  Minor Group Name    Broad Group Name  \n",
       "9  Operations Specialties Managers  Financial Managers  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.iloc[5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e233fc7-5f70-4163-8f67-9e9567eed2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
