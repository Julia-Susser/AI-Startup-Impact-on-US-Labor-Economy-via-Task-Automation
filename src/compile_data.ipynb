{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2daa78e8-e4e8-4ce9-ae99-073277051e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6036 tasks with task similarity > 0.68 and title similarity > 0.6\n",
      "1683 startups out of 2191 decomposed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from llms import gemini\n",
    "from llms import chatGPT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "   \n",
    "def printExamples():\n",
    "    grouped = df.groupby(['organization name', 'website'])\n",
    "    for (org_name, website), group_data in grouped:\n",
    "        print(\"***************************\")\n",
    "        print(f\"Organization: {org_name}, Website: {website}\")\n",
    "        for col in ['generated_description', 'Tasks/Jobs', 'Industry', 'Customers', 'generated_description_conf_interval', 'parsed_description_conf_interval']:\n",
    "            print(f\"{col}: {group_data.iloc[0][col]}\")\n",
    "        for example, example_data in group_data.groupby('example'):\n",
    "            print(\"_________________________\")\n",
    "            print(f\"Example: {example}\")\n",
    "            for col in ['situation_conf_interval', 'situation_conf_interval_reasoning']:\n",
    "                print(f\"{col}: {example_data.iloc[0][col]}\")\n",
    "            for idx, row in example_data.iterrows():\n",
    "                print(\"##########################\")\n",
    "                for col in ['job', 'onet_title', 'onet_task', 'example_job_title', 'task_similarity', 'job_title_similarity', 'onet_weight']:\n",
    "                    print(f\"{col}: {row[col]}\")\n",
    "        print(\"***************************\\n\")\n",
    "        \n",
    "def writeExamples():\n",
    "    output_file_path = \"../output/examples.txt\"\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        grouped = df.groupby(['organization name', 'website'])\n",
    "        for (org_name, website), group_data in grouped:\n",
    "            f.write(\"***************************\\n\")\n",
    "            f.write(f\"Organization: {org_name}, Website: {website}\\n\")\n",
    "            for col in ['generated_description', 'Tasks/Jobs', 'Industry', 'Customers', 'generated_description_conf_interval', 'parsed_description_conf_interval']:\n",
    "                f.write(f\"{col}: {group_data.iloc[0][col]}\\n\")\n",
    "            for example, example_data in group_data.groupby('example'):\n",
    "                f.write(\"_________________________\\n\")\n",
    "                f.write(f\"Example: {example}\\n\")\n",
    "                for col in ['situation_conf_interval', 'situation_conf_interval_reasoning']:\n",
    "                    f.write(f\"{col}: {example_data.iloc[0][col]}\\n\")\n",
    "                for idx, row in example_data.iterrows():\n",
    "                    f.write(\"##########################\\n\")\n",
    "                    for col in ['job', 'onet_title', 'onet_task', 'example_job_title', 'task_similarity', 'job_title_similarity','onet_weight']:\n",
    "                        f.write(f\"{col}: {row[col]}\\n\")\n",
    "            f.write(\"***************************\\n\\n\")\n",
    "                \n",
    "def generateOutput():\n",
    "    conf_interval_text = 'all' #conf_interval if conf_interval != True else 'all'\n",
    "    output_file_path = f\"../output/output_{task_sim}_{title_sim}_{conf_interval_text}.txt\"\n",
    "    num_startups = len(df[\"organization name\"].unique())\n",
    "    num_onet_startups = len(df[\"organization_name\"].unique())\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        group_sums = df.groupby('Minor Group Name')['onet_weight'].sum().sort_values(ascending=False)\n",
    "        f.write(f\"{num_onet_startups} Startups Founded post launch of chatGPT with 1-10 employees\\n\")\n",
    "        \n",
    "        f.write(f\"Task Similarity: {task_sim}, Title Similarity {title_sim}, Confidence Interval: {conf_interval_text}\\n\")\n",
    "        f.write(f\"{num_onet_startups} startups out of {num_startups} decomposed\\n\")\n",
    "        f.write(\"Decomposition of startup effect on labor market\\n\")\n",
    "        \n",
    "        for group_name in group_sums.index:\n",
    "            group_data = df[df['Minor Group Name'] == group_name]\n",
    "            f.write(\"*****************\\n\")\n",
    "            f.write(f\"Group: {group_name}\\n\")\n",
    "            onet_titles = group_data.groupby('onet_title')['onet_weight'].sum().round(2).sort_values(ascending=False).reset_index()\n",
    "            onet_tasks = group_data.groupby('onet_task')['onet_weight'].sum().round(2).sort_values(ascending=False).reset_index()\n",
    "            \n",
    "            f.write(\"________________\\n\")\n",
    "            f.write(\"10 Most Highly Weighted Titles:\\n\")\n",
    "            for index, row in onet_titles.iterrows():\n",
    "                f.write(f\"Title: {row['onet_title']}, Weight: {row['onet_weight']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"10 Most Highly Weighted Tasks:\\n\")\n",
    "            for index, row in list(onet_tasks.iterrows())[:10]:\n",
    "                f.write(f\"Task: {row['onet_task']}, Weight: {row['onet_weight']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"10 Example Startups:\\n\")\n",
    "            startups = group_data.groupby(['organization_name','website'])['onet_weight'].sum().round(2).sort_values(ascending=False).reset_index()\n",
    "            for index, row in list(startups.iterrows())[:10]:\n",
    "                f.write(f\"Startup: {row['organization_name']}, Website: {row['website']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "def get_ONET(task_sim, title_sim, conf_interval=True):\n",
    "    onet_df = pd.read_csv(\"../output/results/onet_df.csv\")\n",
    "    onet_df = onet_df[(onet_df.task_similarity > task_sim) & (onet_df.job_title_similarity > title_sim)]\n",
    "    onet_df = onet_df[onet_df.situation_conf_interval > conf_interval].reset_index(drop=True)\n",
    "    onet_df = compute_ONET_weights(onet_df)\n",
    "    print(f\"{len(onet_df)} tasks with task similarity > {task_sim} and title similarity > {title_sim}\")\n",
    "    return onet_df\n",
    "\n",
    "def compute_ONET_weights(onet_df):\n",
    "    onet_weights = onet_df.groupby(\"organization_name\")[\"onet_title\"].count().apply(lambda x: 1/x).reset_index().rename({\"onet_title\": \"onet_weight\"}, axis=1)\n",
    "    onet_df = onet_df.merge(onet_weights, on=\"organization_name\")\n",
    "    return onet_df\n",
    "\n",
    "\n",
    "def get_startup_generated_LLM_titles(onet_df):\n",
    "    example_job_titles_df = onet_df.example_job_title.value_counts().apply(lambda x: x/3).reset_index()\n",
    "    return example_job_titles_df\n",
    "\n",
    "def print_top_100_ONET_tasks(onet_df):\n",
    "    top_100_ONETtasks = onet_df['onet_task'].value_counts().reset_index().head(100)\n",
    "    top_100_tasks.columns = ['Task', 'Frequency']\n",
    "    for index, row in top_100_tasks.iterrows():\n",
    "        print(f\"Task = {row['Task']}, Frequency = {row['Frequency']}\\n\")\n",
    "\n",
    "def get_startup_data():\n",
    "    startup_df = pd.read_csv(\"../output/results/df_with_examples.csv\")\n",
    "    cols = ['organization name', 'founded date', 'website', 'description_all', 'industries_parsed', 'generated_description', 'parsed_description', 'Tasks/Jobs', 'Industry', 'Customers', 'generated_description_conf_interval', 'parsed_description_conf_interval']\n",
    "    df = startup_df[cols]\n",
    "    df = df.merge(onet_df,left_on=\"organization name\",right_on=\"organization_name\")\n",
    "\n",
    "\n",
    "    onet_occ = pd.read_csv(\"../input/onet/Occupation Data.csv\")[[\"O*NET-SOC Code\",\"Title\"]]\n",
    "    onet_occ.columns = [\"Detailed Occupation\",\"onet_title\"]\n",
    "    df = df.merge(onet_occ,on=\"onet_title\")\n",
    "    df[\"Detailed Occupation\"] = df[\"Detailed Occupation\"].apply(lambda x: x[:-3])\n",
    "    codes = pd.read_csv(\"../input/soc_codes/soc_codes.csv\", index_col=0)\n",
    "    df = df.merge(codes,on=\"Detailed Occupation\", how=\"left\")\n",
    "\n",
    "    \n",
    "    num_startups = len(startup_df[\"organization name\"].unique())\n",
    "    num_onet_startups = len(df[\"organization_name\"].unique())\n",
    "    print(f\"{num_onet_startups} startups out of {num_startups} decomposed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_example_task_embeddings(df):\n",
    "    embedding_df = pd.read_csv('../output/results/df_with_examples_embeddings.csv')\n",
    "    melted_examples = pd.melt(embedding_df, id_vars=['organization name'], value_vars=['Example1', 'Example2', 'Example3'], \n",
    "                        var_name='number', value_name='Example')\n",
    "    \n",
    "    melted_jobs = pd.melt(embedding_df, id_vars=['organization name'], value_vars=['Job1_embedding', 'Job2_embedding', 'Job3_embedding'], \n",
    "                          var_name='number', value_name='example_task_embedding')\n",
    "    melted_jobs.number = melted_jobs.number.apply(lambda x: x[3])\n",
    "    melted_examples.number = melted_examples.number.apply(lambda x: x[-1])\n",
    "    embedding_df = pd.merge(melted_examples, melted_jobs, on=['organization name', 'number'])\n",
    "    df = embedding_df.merge(df,left_on=[\"organization name\",\"Example\"],right_on=[\"organization name\",\"example\"])\n",
    "    return df \n",
    "\n",
    "\n",
    "def get_onet_task_embeddings(df):\n",
    "    df_exp = pd.read_csv('../input/gpts_labels/gpt_exposure_embeddings.csv')\n",
    "    df_exp = df_exp[[x for x in df_exp.columns if \"Unnamed\" not in x]]\n",
    "    df_exp = df_exp[[\"Task\",\"task_embedding\",\"title_embedding\",\"Task ID\"]]\n",
    "    df_exp = df_exp.rename({\"task_embedding\":\"onet_task_embedding\",\"title_embedding\":\"onet_title_embedding\"},axis=1)\n",
    "    df = df.merge(df_exp,left_on=\"onet_task\",right_on=\"Task\",how=\"left\")\n",
    "   \n",
    "    return df\n",
    "    \n",
    "def tsne():\n",
    "    df.example_task_embedding = df.example_task_embedding.apply(lambda x: [float(y) for y in x.strip(\"[]\").split(\", \")])\n",
    "    df.onet_task_embedding = df.onet_task_embedding.apply(lambda x: [float(y) for y in x.strip(\"[]\").split(\", \")])\n",
    "    df.onet_title_embedding = df.onet_title_embedding.apply(lambda x: [float(y) for y in x.strip(\"[]\").split(\", \")])\n",
    "    embeddings = np.vstack(df['example_task_embedding'].values)\n",
    "    tsne = TSNE(n_components=3, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df[[\"example_task_embedding_tsne1\",\"example_task_embedding_tsne2\"]] = tsne_results[:, [0,1]]\n",
    "    \n",
    "    \n",
    "    embeddings = np.vstack(df['onet_task_embedding'].values)\n",
    "    tsne = TSNE(n_components=3, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df[[\"onet_task_embedding_tsne1\",\"onet_task_embedding_tsne2\"]] = tsne_results[:, [0,1]]\n",
    "    \n",
    "    embeddings = np.vstack(df['onet_title_embedding'].values)\n",
    "    tsne = TSNE(n_components=3, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df[[\"onet_title_embedding_tsne1\",\"onet_title_embedding_tsne2\"]] = tsne_results[:, [0,1]]\n",
    "    \n",
    "    tsne = df[['example_task_embedding_tsne1',\n",
    "     'example_task_embedding_tsne2',\n",
    "     'onet_task_embedding_tsne1',\n",
    "     'onet_task_embedding_tsne2',\n",
    "     'onet_title_embedding_tsne1',\n",
    "     'onet_title_embedding_tsne2']]\n",
    "    tsne.to_csv(\"../output/results/tsne.csv\")\n",
    "    return tsne, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "task_sim=0.68\n",
    "title_sim=0.6\n",
    "onet_df = get_ONET(task_sim=0.68, title_sim=0.6)\n",
    "df = get_startup_data()\n",
    "writeExamples()\n",
    "generateOutput()\n",
    "# df = get_example_task_embeddings(df)\n",
    "# df = get_onet_task_embeddings(df)\n",
    "# df.head()\n",
    "# df.to_csv(\"../output/bls_df.csv\")\n",
    "\n",
    "# tsne, df = tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d98cd-138d-461e-b734-696c354e70ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
