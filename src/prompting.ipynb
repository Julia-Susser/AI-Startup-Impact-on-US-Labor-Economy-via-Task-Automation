{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3365a1e-73b1-4f6f-a2cc-b0f226b573e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from os import getenv\n",
    "from openai import OpenAI\n",
    "import pathlib\n",
    "import textwrap\n",
    "# import google.generativeai as genai\n",
    "import time\n",
    "import requests\n",
    "import ast\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from llms import chatGPT\n",
    "from llms import gemini\n",
    "\n",
    "\n",
    "x_chat = chatGPT()\n",
    "x_gemini = gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbadaf6-ce35-481b-9fd5-0e1585e7dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output/data/data_post_chat_gpt.csv\")\n",
    "df = df[df[\"headquarters location\"].str.contains(\"United States\")]\n",
    "df = df[pd.to_datetime(df['founded date'])>='11/30/2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4e9e79-53e6-46ed-9e2e-ca5d89692c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_description_prompt = \"\"\"\n",
    "Your role is to describe $company ($website)'s product. \n",
    "Then, provide a confidence interval on scale on 1-10 on how sure you are about the response. Please be reasonable.\n",
    "\n",
    "PLEASE FOLLOW THE FORMAT EXACTLY FROM THE EXAMPLES!!!!\n",
    "\n",
    "EXAMPLES BELOW\n",
    "************************\n",
    "Company: Genmo\n",
    "Website: https://www.genmo.ai/\n",
    "Description: Genmo is a free tool that creates videos and images using artificial intelligence. Genmo to generate creative text formats of text content, like poems, code, scripts, musical pieces, email, letters, etc.\n",
    "\n",
    "Company: OnePane\n",
    "Website: https://www.onepane.ai/\n",
    "Description: Onepane is a company that offers an AI companion for enhanced DevOps & SRE efficiency. Onepane offers a GenAI solution  providing unparalleled unified insights and control over your Cloud resources. Onepane helps with root cause analysis, cloud governance, and optimization strategies.\n",
    "************************\n",
    "\n",
    "YOUR TURN:\n",
    "Company: $company\n",
    "Wesbite: $website\n",
    "Current Description: $description\n",
    "________________________\n",
    "Description (two sentences):\n",
    "Confidence Interval:\n",
    "Reasoning:\n",
    "________________________\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd94405-75bc-4ea2-babd-8ca39364fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_description_prompt = \"\"\"\n",
    "Your role is to describe what jobs/tasks, industries, and customer that $company is targeting. Then, provide a confidence interval (1-10) on scale on 1-10 on how sure you are about the response. Please be reasonable.\n",
    "\n",
    "A response should include:\n",
    "- tasks/jobs being automated\n",
    "- the industry that the startup applies to\n",
    "- specific customers using the tool. DO NOT INCLUDE AN EXPLANATION\n",
    "\n",
    "PLEASE FOLLOW THE FORMAT EXACTLY FROM THE EXAMPLES!!!!\n",
    "\n",
    "EXAMPLES BELOW\n",
    "************************\n",
    "Company: Petville\n",
    "Website: petville.co/pricing/biz\n",
    "Description: Petville Global is a B2B CRM SaaS platform that utilizes advanced technologies like AI/ML and neural net to streamline and expand operations for pet businesses and veterinary clinics both locally and globally. The platform offers deep data analytics and marketing tools, helping businesses save an average of 22% on CRM and vet tech costs.\n",
    "Tasks/Jobs: Data analysis, Marketing automation, Appointment scheduling, Inventory management\n",
    "Industry: Customer management\n",
    "Customers: Vetinarians, Pet Businesses\n",
    "\n",
    "Company: Thunder\n",
    "Wesbite: thundercompute.com\n",
    "Description: Thunder is a decentralized, peer-to-peer cloud computing platform designed to democratize access to GPUs and address the persistent GPU shortage. It provides a solution for individuals and businesses seeking high-performance computing power, enabling them to leverage unused GPU resources from around the world.\n",
    "Tasks/Jobs: GPU resource allocation, Access to unused GPUs, Distributed computing tasks \n",
    "Industry: Cloud Computing\n",
    "Customers: Developers, GPU Owners\n",
    "\n",
    "Company: NonprofitsHQ\n",
    "Websit: www.nonprofitshq.com\n",
    "Description: NonprofitsHQ is a software suite designed for nonprofits that utilizes AI to automate tasks, manage operations, and improve efficiency, ultimately saving organizations time and resources.\n",
    "Tasks/Jobs: Fundraising management, Donor relationship management, Grant writing, Event planning\n",
    "Industry: Non-profit management\n",
    "Customers: Non-profit organizations\n",
    "************************\n",
    "\n",
    "YOUR TURN:\n",
    "________________________\n",
    "Company: $company\n",
    "Wesbite: $website\n",
    "Description: $description\n",
    "________________________\n",
    "Tasks/Jobs (comma separated list of 4, short):\n",
    "Industry (1 item):\n",
    "Customers (comma separated list): \n",
    "________________________\n",
    "Confidence Interval:\n",
    "Reasoning:\n",
    "________________________\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf9e25a-450b-4fa7-85a2-15cb7fc1499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_prompt = \"\"\"\n",
    "Your role is to provide 3 two-sentence examples of how the product from $company might be used. Do not mention the name of the company in the examples, and keep the descriptions broad.\n",
    "\n",
    "Each example should include:\n",
    "- A detailed description of the job that the tool automates and who performs that job and when.\n",
    "- The ONET job being automated (preferably one from the database https://www.onetonline.org/) and the ONET task that the tool replaces (preferably one that from https://www.onetcenter.org/dictionary/20.1/excel/task_statements.html, include task id)\n",
    "- A confidence interval (1-10) indicating how sure you are about the accuracy of your response.\n",
    "\n",
    "PLEASE FOLLOW THE FORMAT EXACTLY FROM THE EXAMPLES!!!!\n",
    "\n",
    "The goal is to map each example back to ONET jobs. If the job being automated is recognized by ONET, please use the ONET job title. \n",
    "If it is not typically found in ONET, use best judgement!\n",
    "\n",
    "EXAMPLES BELOW: \n",
    "************************\n",
    "Company: Blanc\n",
    "Website: tryblanc.ai\n",
    "Blanc is a compliance automation platform designed for fintech companies. It helps streamline regulatory compliance processes by providing a centralized hub for managing policies, monitoring activities, and generating reports.\n",
    "_________________________\n",
    "Example 1: A compliance officer at a fintech company uses Blanc to automate the process of creating and updating compliance policies, ensuring all documents are current and accessible to relevant team members.\n",
    "ONET JOB automated 1: Compliance Officers that verify that all firm and regulatory policies and procedures have been documented, implemented, and communicated.\n",
    "ONET JOB 1: Compliance Officers\n",
    "_________________________\n",
    "Example 2: A fintech company uses Blanc to generate automated compliance reports for regulatory audits, ensuring all necessary documentation is readily available and organized.\n",
    "ONET JOB automated 2: Compliance Officers Prepare reports of activities, evaluations, recommendations, or decisions.\n",
    "ONET JOB 2: Compliance Officers\n",
    "_________________________\n",
    "Example 3: A fintech company uses Blanc to monitor real-time transactions for potential compliance violations, triggering alerts and generating reports for further investigation.\n",
    "ONET JOB automated 3: Compliance Officers that identify compliance issues that require follow-up or investigation.\n",
    "ONET JOB 3: Compliance Officers\n",
    "\n",
    "\n",
    "\n",
    "Company: Aether\n",
    "Website: aetherenergie.com/\n",
    "Aether Energy is an AI-driven platform designed to simplify the process of rooftop solar installation for businesses, providing comprehensive support from project planning and financing to installation and ongoing maintenance. This platform aims to streamline and optimize the entire solar energy journey for installers. \n",
    "________________\n",
    "Example 1: A solar installer uses Aether to quickly create detailed project plans for rooftop solar installations, including system size, panel placement, and wiring diagrams. \n",
    "ONET JOB automated 1: Solar Photovoltaic Installers that diagram layouts and locations for photovoltaic (PV) arrays and equipment, including existing building or site features.\n",
    "ONET JOB 1: Solar Photovoltaic Installers\n",
    "________________\n",
    "Example 2: A business owner leverages Aether to secure financing for their rooftop solar project, providing them with customized loan options and streamlined application processes. \n",
    "ONET JOB automated 2: Solar Photovoltaic Installers that prepare solar installation project proposals, quotes, budgets, or schedules.\n",
    "ONET JOB 2: Solar Photovoltaic Installers\n",
    "________________\n",
    "Example 3: A solar installer uses Aether to manage the installation process, tracking materials, scheduling technicians, and coordinating with subcontractors, ensuring smooth project execution. \n",
    "ONET JOB automated 3:  Solar Engery Installation Managers that monitor work of contractors and subcontractors to ensure projects conform to plans, specifications, schedules, or budgets.\n",
    "ONET JOB 3: Solar Energy Installation Managers\n",
    "________________\n",
    "************************\n",
    "\n",
    "YOUR TURN:\n",
    "Company:$company\n",
    "Website: $website\n",
    "Current Description: $description\n",
    "$parsed_description\n",
    "________________\n",
    "Example 1: \n",
    "ONET JOB automated 1:\n",
    "ONET JOB 1: \n",
    "Confidence Interval 1:\n",
    "Reasoning 1:\n",
    "________________\n",
    "Example 2: \n",
    "ONET JOB automated 2:\n",
    "ONET JOB 2: \n",
    "Confidence Interval 2:\n",
    "Reasoning 2:\n",
    "________________\n",
    "Example 3:\n",
    "ONET JOB automated 3:\n",
    "ONET JOB 3: \n",
    "Confidence Interval 3:\n",
    "Reasoning 3:\n",
    "________________\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75f31840-edb9-4c8b-89f5-6b4a1b813dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class prompting():\n",
    "    def __init__(self):\n",
    "        self.results_df = pd.DataFrame(columns=[\"organization name\", \"value\"])\n",
    "\n",
    "    def set_current_results_df(self, results_df):\n",
    "        self.results_df = results_df\n",
    "\n",
    "    async def iterate(self, df, prompt_template, args, value, batch_size=10, start=0, end=False):\n",
    "        if end == False:\n",
    "            end = len(df)\n",
    "        self.results_df = pd.DataFrame(columns=[\"organization name\", value])\n",
    "        if value in list(df.columns):\n",
    "            if start != 0:\n",
    "                self.results_df = pd.concat([df[[\"organization name\", value]].iloc[:start], self.results_df], axis=0)\n",
    "            df = df.drop(columns=[value])\n",
    "\n",
    "        batch_prompts = []\n",
    "        batch_indices = []\n",
    "\n",
    "        for i, row in list(df.iterrows())[start:end]:\n",
    "            name = row['organization name']\n",
    "            website = row['website']\n",
    "            prompt = prompt_template\n",
    "\n",
    "            for arg in args:\n",
    "                prompt = prompt.replace(f\"${arg[0]}\", row[arg[1]])\n",
    "\n",
    "            batch_prompts.append((i, name, prompt))\n",
    "            batch_indices.append(i)\n",
    "\n",
    "            if len(batch_prompts) >= batch_size:\n",
    "                await self.process_batch(batch_prompts)\n",
    "                batch_prompts = []\n",
    "\n",
    "        if batch_prompts:\n",
    "            await self.process_batch(batch_prompts)\n",
    "            self.results_df.to_csv(\"../output/current_results_df_prompting.csv\")\n",
    "\n",
    "        df = df.merge(self.results_df, on='organization name', how='left')\n",
    "        return df\n",
    "\n",
    "    async def process_batch(self, batch_prompts):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for i, name, prompt in batch_prompts:\n",
    "                tasks.append(self.fetch_result(session, i, name, prompt))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    async def fetch_result(self, session, i, name, prompt):\n",
    "        failure_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                print(f\"******************************\\nProcessing {i}: {name}\")\n",
    "                result = await x_gemini.ask(session, prompt)\n",
    "                if result == \"N/A\": break  # explicit material\n",
    "\n",
    "                text = re.sub(r\"#|#\\s+|_|\\*\", \"\", result).strip()\n",
    "\n",
    "                self.results_df.loc[i] = [name, text]\n",
    "\n",
    "                    \n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(failure_count)\n",
    "                failure_count += 1\n",
    "                if failure_count > 10:\n",
    "                    break\n",
    "                print(f\"Error processing {i}, {name}: {e}\")\n",
    "                await asyncio.sleep(20)\n",
    "\n",
    "prompting_class = prompting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f708d1c-1ef0-4edd-9da7-0e0684d44bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Processing 0: Pika\n",
      "******************************\n",
      "Processing 1: Contextual AI\n",
      "******************************\n",
      "Processing 2: Sierra\n",
      "******************************\n",
      "Processing 3: Essential AI\n",
      "******************************\n",
      "Processing 4: Liquid AI\n",
      "******************************\n",
      "Processing 5: Lindy\n",
      "******************************\n",
      "Processing 6: LlamaIndex\n",
      "******************************\n",
      "Processing 7: MagicSchool AI\n",
      "******************************\n",
      "Processing 8: Norm AI\n",
      "******************************\n",
      "Processing 9: Patronus AI\n",
      "******************************\n",
      "Processing 10: Sixfold AI\n",
      "******************************\n",
      "Processing 11: Voltage Park\n",
      "******************************\n",
      "Processing 12: Elicit\n",
      "******************************\n",
      "Processing 13: Mercor\n",
      "******************************\n",
      "Processing 14: Guardrails AI\n",
      "******************************\n",
      "Processing 15: VectorShift\n",
      "******************************\n",
      "Processing 16: Portkey AI\n",
      "******************************\n",
      "Processing 17: MyShell\n",
      "******************************\n",
      "Processing 18: Genmo\n",
      "******************************\n",
      "Processing 19: Braintrust Data\n",
      "******************************\n",
      "Processing 20: Paxton AI\n",
      "******************************\n",
      "Processing 21: Oliv AI\n",
      "******************************\n",
      "Processing 22: Mindtrip\n",
      "******************************\n",
      "Processing 23: Worth AI\n",
      "******************************\n",
      "Processing 24: Kintsugi AI\n",
      "******************************\n",
      "Processing 25: Greenlite\n",
      "******************************\n",
      "Processing 26: Arkifi\n",
      "******************************\n",
      "Processing 27: Retell AI\n",
      "******************************\n",
      "Processing 28: LastMile AI\n",
      "******************************\n",
      "Processing 29: Rhythms\n",
      "******************************\n",
      "Processing 30: Stack AI\n",
      "******************************\n",
      "Processing 31: Sesame AI\n",
      "******************************\n",
      "Processing 32: Prins Artificial Intelligence\n",
      "******************************\n",
      "Processing 33: Echo Labs\n",
      "******************************\n",
      "Processing 34: Nexusflow\n",
      "******************************\n",
      "Processing 35: Workhack\n",
      "******************************\n",
      "Processing 36: Prompt Security\n",
      "******************************\n",
      "Processing 37: Distributional\n",
      "******************************\n",
      "Processing 38: Leap AI\n",
      "******************************\n",
      "Processing 39: GenHealth.ai\n",
      "******************************\n",
      "Processing 40: Class Companion\n",
      "******************************\n",
      "Processing 41: Procurement Sciences AI\n",
      "******************************\n",
      "Processing 42: Socap.ai\n",
      "******************************\n",
      "Processing 43: Meeno\n",
      "******************************\n",
      "Processing 44: FirmPilot\n",
      "******************************\n",
      "Processing 45: Arcus\n",
      "******************************\n",
      "Processing 46: RadiantGraph\n",
      "******************************\n",
      "Processing 47: HydroX AI\n",
      "******************************\n",
      "Processing 48: AiSDR\n",
      "******************************\n",
      "Processing 49: Codegen\n",
      "******************************\n",
      "Processing 50: Mindy\n",
      "******************************\n",
      "Processing 51: XponentL Data\n",
      "******************************\n",
      "Processing 52: Xylem AI\n",
      "******************************\n",
      "Processing 53: dili\n",
      "******************************\n",
      "Processing 54: Prophetic AI\n",
      "******************************\n",
      "Processing 55: Harmonic Security\n",
      "******************************\n",
      "Processing 56: Series.AI\n",
      "******************************\n",
      "Processing 57: Basis\n",
      "******************************\n",
      "Processing 58: HeyMilo\n",
      "******************************\n",
      "Processing 59: Gradial\n",
      "******************************\n",
      "Processing 60: Zep AI\n",
      "******************************\n",
      "Processing 61: Teragonia\n",
      "******************************\n",
      "Processing 62: LighthouseAI\n",
      "******************************\n",
      "Processing 63: Lutra AI\n",
      "******************************\n",
      "Processing 64: Delphina\n",
      "******************************\n",
      "Processing 65: Metal\n",
      "******************************\n",
      "Processing 66: Catio\n",
      "******************************\n",
      "Processing 67: Spiffy AI\n",
      "******************************\n",
      "Processing 68: Giga ML\n",
      "******************************\n",
      "Processing 69: Vieu\n",
      "******************************\n",
      "Processing 70: Automat AI\n",
      "******************************\n",
      "Processing 71: Invoke AI\n",
      "******************************\n",
      "Processing 72: SapientAI\n",
      "******************************\n",
      "Processing 73: Doowii, Inc\n",
      "******************************\n",
      "Processing 74: Responsiv\n",
      "******************************\n",
      "Processing 75: Defog\n",
      "******************************\n",
      "Processing 76: Terminal Industries\n",
      "******************************\n",
      "Processing 77: Finpilot\n",
      "******************************\n",
      "Processing 78: Lume\n",
      "******************************\n",
      "Processing 79: Helicone\n",
      "******************************\n",
      "Processing 80: Gushwork\n",
      "******************************\n",
      "Processing 81: Handraise\n",
      "******************************\n",
      "Processing 82: Yuma AI\n",
      "******************************\n",
      "Processing 83: Offered.ai\n",
      "******************************\n",
      "Processing 84: Reworkd\n",
      "******************************\n",
      "Processing 85: Foundation LLM\n",
      "******************************\n",
      "Processing 86: Hitloop\n",
      "******************************\n",
      "Processing 87: FORA\n",
      "******************************\n",
      "Processing 88: Dapta\n",
      "******************************\n",
      "Processing 89: Hatz AI\n",
      "******************************\n",
      "Processing 90: Prompt AI\n",
      "******************************\n",
      "Processing 91: Sweep\n",
      "******************************\n",
      "Processing 92: Clueso\n",
      "******************************\n",
      "Processing 93: Superframe\n",
      "******************************\n",
      "Processing 94: TabbyML\n",
      "******************************\n",
      "Processing 95: Crowda\n",
      "******************************\n",
      "Processing 96: GenLogs\n",
      "******************************\n",
      "Processing 97: Cortex\n",
      "******************************\n",
      "Processing 98: Rock Rabbit\n",
      "******************************\n",
      "Processing 99: Heliux\n",
      "******************************\n",
      "Processing 100: Berri AI\n",
      "******************************\n",
      "Processing 101: Asato\n",
      "******************************\n",
      "Processing 102: Brand.AI\n",
      "******************************\n",
      "Processing 103: Haltia.AI\n",
      "******************************\n",
      "Processing 104: Trustible\n",
      "******************************\n",
      "Processing 105: Pathways Technologies, Inc.\n",
      "******************************\n",
      "Processing 106: OkamiAI\n",
      "******************************\n",
      "Processing 107: Bobyard\n",
      "******************************\n",
      "Processing 108: E2B\n",
      "******************************\n",
      "Processing 109: Tailbox\n",
      "******************************\n",
      "Processing 110: Aimerce\n",
      "******************************\n",
      "Processing 111: Deasie\n",
      "******************************\n",
      "Processing 112: Era Finance\n",
      "******************************\n",
      "Processing 113: WorkMagic\n",
      "******************************\n",
      "Processing 114: Maximal Learning\n",
      "******************************\n",
      "Processing 115: UnityAI\n",
      "******************************\n",
      "Processing 116: Adrenaline\n",
      "******************************\n",
      "Processing 117: Persana AI\n",
      "******************************\n",
      "Processing 118: Berry\n",
      "******************************\n",
      "Processing 119: Talus Network\n",
      "******************************\n",
      "Processing 120: Rengage\n",
      "******************************\n",
      "Processing 121: Sensigo\n",
      "******************************\n",
      "Processing 122: PolyAPI\n",
      "******************************\n",
      "Processing 123: nutrƒÅd\n",
      "******************************\n",
      "Processing 124: Spine AI\n",
      "******************************\n",
      "Processing 125: Harmony Games\n",
      "******************************\n",
      "Processing 126: Helios Artificial Intelligence\n",
      "******************************\n",
      "Processing 127: Bookend AI\n",
      "******************************\n",
      "Processing 128: Monster API\n",
      "******************************\n",
      "Processing 129: Salespeak\n",
      "******************************\n",
      "Processing 130: Fintool.com\n",
      "******************************\n",
      "Processing 131: forml\n",
      "******************************\n",
      "Processing 132: Kontigo\n",
      "******************************\n",
      "Processing 133: Lex\n",
      "******************************\n",
      "Processing 134: SERV\n",
      "******************************\n",
      "Processing 135: Illumicell AI\n",
      "******************************\n",
      "Processing 136: Parea AI\n",
      "******************************\n",
      "Processing 137: Vectari\n",
      "******************************\n",
      "Processing 138: BoolSi\n",
      "******************************\n",
      "Processing 139: Fume\n",
      "******************************\n",
      "Processing 140: Multiplayer\n",
      "******************************\n",
      "Processing 141: Code Metal\n",
      "******************************\n",
      "Processing 142: Knownwell\n",
      "******************************\n",
      "Processing 143: Beagle\n",
      "******************************\n",
      "Processing 144: Visionify\n",
      "******************************\n",
      "Processing 145: Keywords AI\n",
      "******************************\n",
      "Processing 146: Decoda Health\n",
      "******************************\n",
      "Processing 147: Newton Research\n",
      "******************************\n",
      "Processing 148: Synfini\n",
      "******************************\n",
      "Processing 149: BitSync\n",
      "******************************\n",
      "Processing 150: Mentium.io\n",
      "******************************\n",
      "Processing 151: Helix by HL\n",
      "******************************\n",
      "Processing 152: UltronAI\n",
      "******************************\n",
      "Processing 153: One Kappa\n",
      "******************************\n",
      "Processing 154: Wealthfeed\n",
      "******************************\n",
      "Processing 155: Salient\n",
      "******************************\n",
      "Processing 156: Pixeland Technologies, Inc.\n",
      "******************************\n",
      "Processing 157: Pull Systems\n",
      "******************************\n",
      "Processing 158: Meru\n",
      "******************************\n",
      "Processing 159: Pliable\n",
      "******************************\n",
      "Processing 160: Martin\n",
      "******************************\n",
      "Processing 161: CodeParrot\n",
      "******************************\n",
      "Processing 162: primeclass.ai\n",
      "******************************\n",
      "Processing 163: WAVs AI\n",
      "******************************\n",
      "Processing 164: Essence App\n",
      "******************************\n",
      "Processing 165: Surgical Automations\n",
      "******************************\n",
      "Processing 166: Optain\n",
      "******************************\n",
      "Processing 167: Tabor.AI\n",
      "******************************\n",
      "Processing 168: Quiller\n",
      "******************************\n",
      "Processing 169: Sunrise\n",
      "******************************\n",
      "Processing 170: Merit Medicine\n",
      "******************************\n",
      "Processing 171: Context Nature\n",
      "******************************\n",
      "Processing 172: ContextQA\n",
      "******************************\n",
      "Processing 173: Preloop\n",
      "******************************\n",
      "Processing 174: Baseplate\n",
      "******************************\n",
      "Processing 175: Osmos\n",
      "******************************\n",
      "Processing 176: Twine\n",
      "******************************\n",
      "Processing 177: Industrial Data Labs\n",
      "******************************\n",
      "Processing 178: AiFlow\n",
      "******************************\n",
      "Processing 179: Pam\n",
      "******************************\n",
      "Processing 180: OpenAds\n",
      "******************************\n",
      "Processing 181: Innkeeper\n",
      "******************************\n",
      "Processing 182: Osmo\n",
      "******************************\n",
      "Processing 183: Bench AI\n",
      "******************************\n",
      "Processing 184: Steno.ai\n",
      "******************************\n",
      "Processing 185: BlueBean\n",
      "******************************\n",
      "Processing 186: AQX\n",
      "******************************\n",
      "Processing 187: Charmed\n",
      "******************************\n",
      "Processing 188: Plai Labs\n",
      "******************************\n",
      "Processing 189: Edifii\n",
      "******************************\n",
      "Processing 190: HoneyGrid\n",
      "******************************\n",
      "Processing 191: SuperStorm\n",
      "******************************\n",
      "Processing 192: Cohere Commerce\n",
      "******************************\n",
      "Processing 193: Readout AI\n",
      "******************************\n",
      "Processing 194: Coala Pay\n",
      "******************************\n",
      "Processing 195: Atacama Biomaterials\n",
      "******************************\n",
      "Processing 196: TalkStack AI\n",
      "******************************\n",
      "Processing 197: Jounce\n",
      "******************************\n",
      "Processing 198: Algorized\n",
      "******************************\n",
      "Processing 199: Pelles.ai\n",
      "******************************\n",
      "Processing 200: Wild Moose\n",
      "******************************\n",
      "Processing 201: Hyper\n",
      "******************************\n",
      "Processing 202: SmartWiz\n",
      "******************************\n",
      "Processing 203: ProspectStream Software, Inc\n",
      "******************************\n",
      "Processing 204: Ezyhire\n",
      "******************************\n",
      "Processing 205: Target Eagle\n",
      "******************************\n",
      "Processing 206: Sero AI\n",
      "******************************\n",
      "Processing 207: GOLDFIRE\n",
      "******************************\n",
      "Processing 208: Narrative\n",
      "******************************\n",
      "Processing 209: Lightski\n",
      "******************************\n",
      "Processing 210: BlogSEO AI\n",
      "******************************\n",
      "Processing 211: AutoSquared\n",
      "******************************\n",
      "Processing 212: Vaero\n",
      "******************************\n",
      "Processing 213: Digital Will & Trust\n",
      "******************************\n",
      "Processing 214: AugMend\n",
      "******************************\n",
      "Processing 215: Quasar Markets\n",
      "******************************\n",
      "Processing 216: Cusman\n",
      "******************************\n",
      "Processing 217: Texel.ai\n",
      "******************************\n",
      "Processing 218: Speck\n",
      "******************************\n",
      "Processing 219: Fig Medical\n",
      "******************************\n",
      "Processing 220: Testaify\n",
      "******************************\n",
      "Processing 221: BabylonAI\n",
      "******************************\n",
      "Processing 222: Esger\n",
      "******************************\n",
      "Processing 223: Screenify\n",
      "******************************\n",
      "Processing 224: CTFGuide\n",
      "******************************\n",
      "Processing 225: HigherU\n",
      "******************************\n",
      "Processing 226: Vestinda\n",
      "******************************\n",
      "Processing 227: R&A Data\n",
      "******************************\n",
      "Processing 228: Agent Copilot\n",
      "******************************\n",
      "Processing 229: Medical Innovation Centers of America, INC.\n",
      "******************************\n",
      "Processing 230: RoleScale, Inc\n",
      "******************************\n",
      "Processing 231: Vocadian\n",
      "******************************\n",
      "Processing 232: Logic Overdrive\n",
      "******************************\n",
      "Processing 233: UpSide\n",
      "******************************\n",
      "Processing 234: LinkGrep\n",
      "******************************\n",
      "Processing 235: Prospero\n",
      "******************************\n",
      "Processing 236: Maven AGI\n",
      "******************************\n",
      "Processing 237: ImprezzAI\n",
      "******************************\n",
      "Processing 238: ClaroAI\n",
      "******************************\n",
      "Processing 239: Paraplanner\n",
      "******************************\n",
      "Processing 240: Prep Intel\n",
      "******************************\n",
      "Processing 241: Et Cetera Robotics\n",
      "******************************\n",
      "Processing 242: Lepton AI\n",
      "******************************\n",
      "Processing 243: WePlan\n",
      "******************************\n",
      "Processing 244: Trapol8\n",
      "******************************\n",
      "Processing 245: EFAS Technologies\n",
      "******************************\n",
      "Processing 246: Semantic Finance\n",
      "******************************\n",
      "Processing 247: Mimrr\n",
      "******************************\n",
      "Processing 248: Motivision\n",
      "******************************\n",
      "Processing 249: Pezzi AI\n",
      "******************************\n",
      "Processing 250: Potato\n",
      "******************************\n",
      "Processing 251: admin AI\n",
      "******************************\n",
      "Processing 252: Voyage Technologies\n",
      "******************************\n",
      "Processing 253: SneakPeek\n",
      "******************************\n",
      "Processing 254: LucciAI\n",
      "******************************\n",
      "Processing 255: Modlee\n",
      "******************************\n",
      "Processing 256: e11tec\n",
      "******************************\n",
      "Processing 257: Chi AI\n",
      "******************************\n",
      "Processing 258: Future State University\n",
      "******************************\n",
      "Processing 259: FameFlow\n",
      "******************************\n",
      "Processing 260: Bummock AI\n",
      "******************************\n",
      "Processing 261: CivicSync\n",
      "******************************\n",
      "Processing 262: Moonlight\n",
      "******************************\n",
      "Processing 263: Core Ally\n",
      "******************************\n",
      "Processing 264: PeoplePlus\n",
      "******************************\n",
      "Processing 265: VenariX\n",
      "******************************\n",
      "Processing 266: Salee\n",
      "******************************\n",
      "Processing 267: Candle\n",
      "******************************\n",
      "Processing 268: Materian.AI\n",
      "******************************\n",
      "Processing 269: DivySci\n",
      "******************************\n",
      "Processing 270: Homescore AI\n",
      "******************************\n",
      "Processing 271: RadUnity\n",
      "******************************\n",
      "Processing 272: Overland AI\n",
      "******************************\n",
      "Processing 273: CatchGPT\n",
      "******************************\n",
      "Processing 274: Spot Runner AI\n",
      "******************************\n",
      "Processing 275: ARTI Analytics\n",
      "******************************\n",
      "Processing 276: Oblio\n",
      "******************************\n",
      "Processing 277: Fresho\n",
      "******************************\n",
      "Processing 278: Shophand\n",
      "******************************\n",
      "Processing 279: Izx\n",
      "******************************\n",
      "Processing 280: HUMN Capital\n",
      "******************************\n",
      "Processing 281: MedDefend\n",
      "******************************\n",
      "Processing 282: askMay\n",
      "******************************\n",
      "Processing 283: JARS AI\n",
      "******************************\n",
      "Processing 284: OpenPipe\n",
      "******************************\n",
      "Processing 285: DemTEC\n",
      "******************************\n",
      "Processing 286: ColdAI\n",
      "******************************\n",
      "Processing 287: Archetype AI\n",
      "******************************\n",
      "Processing 288: Log10\n",
      "******************************\n",
      "Processing 289: FairNow\n",
      "******************************\n",
      "Processing 290: Influsence\n",
      "******************************\n",
      "Processing 291: CloudShore\n",
      "******************************\n",
      "Processing 292: Wizybot\n",
      "******************************\n",
      "Processing 293: Multimodal\n",
      "******************************\n",
      "Processing 294: Cassidy\n",
      "******************************\n",
      "Processing 295: Cosmic Robotics\n",
      "******************************\n",
      "Processing 296: Letter AI\n",
      "******************************\n",
      "Processing 297: Medbill AI\n",
      "******************************\n",
      "Processing 298: Assort Health\n",
      "******************************\n",
      "Processing 299: Marveri\n",
      "******************************\n",
      "Processing 300: DataMotto\n",
      "******************************\n",
      "Processing 301: Magic Hour\n",
      "******************************\n",
      "Processing 302: Tradespect\n",
      "******************************\n",
      "Processing 303: Inkeep\n",
      "******************************\n",
      "Processing 304: Autoblocks AI\n",
      "******************************\n",
      "Processing 305: Inner AI\n",
      "******************************\n",
      "Processing 306: Mika Health\n",
      "******************************\n",
      "Processing 307: Ontinue\n",
      "******************************\n",
      "Processing 308: Blox\n",
      "******************************\n",
      "Processing 309: EcoRatings\n",
      "******************************\n",
      "Processing 310: Quizard AI\n",
      "******************************\n",
      "Processing 311: Spresso\n",
      "******************************\n",
      "Processing 312: Aethero\n",
      "******************************\n",
      "Processing 313: MediSearch\n",
      "******************************\n",
      "Processing 314: Trumio\n",
      "******************************\n",
      "Processing 315: Sola\n",
      "******************************\n",
      "Processing 316: Caju AI\n",
      "******************************\n",
      "Processing 317: Anakin AI\n",
      "******************************\n",
      "Processing 318: Firsthand\n",
      "******************************\n",
      "Processing 319: Uptime Industries\n",
      "******************************\n",
      "Processing 320: Kobalt Labs\n",
      "******************************\n",
      "Processing 321: Ambient\n",
      "******************************\n",
      "Processing 322: Relm\n",
      "******************************\n",
      "Processing 323: Revocalize AI\n",
      "******************************\n",
      "Processing 324: Constructable\n",
      "******************************\n",
      "Processing 325: Butternut AI\n",
      "******************************\n",
      "Processing 326: Veltris\n",
      "******************************\n",
      "Processing 327: Kind Humanoid\n",
      "******************************\n",
      "Processing 328: Cerbrec\n",
      "******************************\n",
      "Processing 329: Shadeform AI\n",
      "******************************\n",
      "Processing 330: Twee\n",
      "******************************\n",
      "Processing 331: Flotation Innovation\n",
      "******************************\n",
      "Processing 332: Synthetica Bio\n",
      "******************************\n",
      "Processing 333: Momentic\n",
      "******************************\n",
      "Processing 334: Glaze\n",
      "******************************\n",
      "Processing 335: AI Lawyer\n",
      "******************************\n",
      "Processing 336: Cimphony\n",
      "******************************\n",
      "Processing 337: Cedana\n",
      "******************************\n",
      "Processing 338: Diagnose Early\n",
      "******************************\n",
      "Processing 339: bitHuman\n",
      "******************************\n",
      "Processing 340: FinanceOps.AI\n",
      "******************************\n",
      "Processing 341: Maia\n",
      "******************************\n",
      "Processing 342: Arkham Technologies\n",
      "******************************\n",
      "Processing 343: NeuralFabric\n",
      "******************************\n",
      "Processing 344: InfraHive\n",
      "******************************\n",
      "Processing 345: Keebler Health\n",
      "******************************\n",
      "Processing 346: Olio Labs\n",
      "******************************\n",
      "Processing 347: Trust Fund\n",
      "******************************\n",
      "Processing 348: Corgea\n",
      "******************************\n",
      "Processing 349: Eximietas Design\n",
      "******************************\n",
      "Processing 350: Ask Sage\n",
      "******************************\n",
      "Processing 351: LemonRocks\n",
      "******************************\n",
      "Processing 352: Khoj\n",
      "******************************\n",
      "Processing 353: Naro\n",
      "******************************\n",
      "Processing 354: Flamme AI - The Couples App\n",
      "******************************\n",
      "Processing 355: Quill AI\n",
      "******************************\n",
      "Processing 356: Tely AI\n",
      "******************************\n",
      "Processing 357: Letz Technologies\n",
      "******************************\n",
      "Processing 358: Entrada\n",
      "******************************\n",
      "Processing 359: Nara\n",
      "******************************\n",
      "Processing 360: Myriad Venture Partners\n",
      "******************************\n",
      "Processing 361: askLio AI\n",
      "******************************\n",
      "Processing 362: Remy Security\n",
      "******************************\n",
      "Processing 363: Layerpath\n",
      "******************************\n",
      "Processing 364: CandorIQ\n",
      "******************************\n",
      "Processing 365: Magic Loops\n",
      "******************************\n",
      "Processing 366: ZippiAi\n",
      "******************************\n",
      "Processing 367: Superintelligence\n",
      "******************************\n",
      "Processing 368: Vatic AI LLC\n",
      "******************************\n",
      "Processing 369: Linc AI\n",
      "******************************\n",
      "Processing 370: CodeComplete\n",
      "******************************\n",
      "Processing 371: Kamoto.AI\n",
      "******************************\n",
      "Processing 372: Haxion\n",
      "******************************\n",
      "Processing 373: Avenue Z\n",
      "******************************\n",
      "Processing 374: Remyx AI\n",
      "******************************\n",
      "Processing 375: sudocode\n",
      "******************************\n",
      "Processing 376: ApplyPass.com\n",
      "******************************\n",
      "Processing 377: Imageryst\n",
      "******************************\n",
      "Processing 378: Dataspan AI\n",
      "******************************\n",
      "Processing 379: PhotoAi\n",
      "******************************\n",
      "Processing 380: Foresight Data\n",
      "******************************\n",
      "Processing 381: Atrix\n",
      "******************************\n",
      "Processing 382: Orchestra AI\n",
      "******************************\n",
      "Processing 383: Smart Retrieval\n",
      "******************************\n",
      "Processing 384: Talc AI\n",
      "******************************\n",
      "Processing 385: Harmony\n",
      "******************************\n",
      "Processing 386: Postilize\n",
      "******************************\n",
      "Processing 387: Infobot.ai\n",
      "******************************\n",
      "Processing 388: Mano AI\n",
      "******************************\n",
      "Processing 389: EyeTell\n",
      "******************************\n",
      "Processing 390: Jex\n",
      "******************************\n",
      "Processing 391: Cerelyze\n",
      "******************************\n",
      "Processing 392: VideoGen\n",
      "******************************\n",
      "Processing 393: Augrade\n",
      "******************************\n",
      "Processing 394: UVIONIX\n",
      "******************************\n",
      "Processing 395: LoopGenius\n",
      "******************************\n",
      "Processing 396: Skyworker.ai\n",
      "******************************\n",
      "Processing 397: SOMMS.AI\n",
      "******************************\n",
      "Processing 398: ThoughtLinks\n",
      "******************************\n",
      "Processing 399: Blackwire Labs\n",
      "******************************\n",
      "Processing 400: bixod\n",
      "******************************\n",
      "Processing 401: SupportFinity\n",
      "******************************\n",
      "Processing 402: GPTConsole\n",
      "******************************\n",
      "Processing 403: Nicky AI\n",
      "******************************\n",
      "Processing 404: Wyvern AI\n",
      "******************************\n",
      "Processing 405: Lind AI\n",
      "******************************\n",
      "Processing 406: DocPlace\n",
      "******************************\n",
      "Processing 407: extrakt.AI\n",
      "******************************\n",
      "Processing 408: Livvy\n",
      "******************************\n",
      "Processing 409: AgentX\n",
      "******************************\n",
      "Processing 410: Brainchain AI\n",
      "******************************\n",
      "Processing 411: Autentik AI\n",
      "******************************\n",
      "Processing 412: Hauska\n",
      "******************************\n",
      "Processing 413: Commonbase\n",
      "******************************\n",
      "Processing 414: Sobo\n",
      "******************************\n",
      "Processing 415: SafeVideo AI\n",
      "******************************\n",
      "Processing 416: FleetWorks Technology\n",
      "******************************\n",
      "Processing 417: Celeste\n",
      "******************************\n",
      "Processing 418: Invista Health\n",
      "******************************\n",
      "Processing 419: Strings\n",
      "******************************\n",
      "Processing 420: ColentAI\n",
      "******************************\n",
      "Processing 421: Payfederate\n",
      "******************************\n",
      "Processing 422: Nine Minds\n",
      "******************************\n",
      "Processing 423: Swayware\n",
      "******************************\n",
      "Processing 424: Spectio\n",
      "******************************\n",
      "Processing 425: Astros AI\n",
      "******************************\n",
      "Processing 426: Imagine AI\n",
      "******************************\n",
      "Processing 427: Artizia\n",
      "******************************\n",
      "Processing 428: Shubox\n",
      "******************************\n",
      "Processing 429: Bikia Health\n",
      "******************************\n",
      "Processing 430: Notable AI\n",
      "******************************\n",
      "Processing 431: Legal Hat\n",
      "******************************\n",
      "Processing 432: Cantaloupe AI\n",
      "******************************\n",
      "Processing 433: Kahuna AI\n",
      "******************************\n",
      "Processing 434: Ripple AI\n",
      "******************************\n",
      "Processing 435: 4Eyes.ai\n",
      "******************************\n",
      "Processing 436: Jobsolv\n",
      "******************************\n",
      "Processing 437: Mach\n",
      "******************************\n",
      "Processing 438: Echelon AI\n",
      "******************************\n",
      "Processing 439: Dono\n",
      "******************************\n",
      "Processing 440: Hover\n",
      "******************************\n",
      "Processing 441: Intellevo AI\n",
      "******************************\n",
      "Processing 442: Intelliga Health\n",
      "******************************\n",
      "Processing 443: Context\n",
      "******************************\n",
      "Processing 444: Kopius\n",
      "******************************\n",
      "Processing 445: H9 Technologies\n",
      "******************************\n",
      "Processing 446: EnlightenAI\n",
      "******************************\n",
      "Processing 447: Starseed AI\n",
      "******************************\n",
      "Processing 448: Turu\n",
      "******************************\n",
      "Processing 449: AfroHealth\n",
      "******************************\n",
      "Processing 450: IsWorky\n",
      "******************************\n",
      "Processing 451: Fincraft\n",
      "******************************\n",
      "Processing 452: Oikeus.AI\n",
      "******************************\n",
      "Processing 453: goML\n",
      "******************************\n",
      "Processing 454: Perception\n",
      "******************************\n",
      "Processing 455: User Evaluation\n",
      "******************************\n",
      "Processing 456: YORG AI\n",
      "******************************\n",
      "Processing 457: Creative.ai\n",
      "******************************\n",
      "Processing 458: Move AI\n",
      "******************************\n",
      "Processing 459: Olympus\n",
      "******************************\n",
      "Processing 460: Thruways\n",
      "******************************\n",
      "Processing 461: EntryDock\n",
      "******************************\n",
      "Processing 462: TidalWave\n",
      "******************************\n",
      "Processing 463: Batteryze\n",
      "******************************\n",
      "Processing 464: Anjo.ai\n",
      "******************************\n",
      "Processing 465: Runmic\n",
      "******************************\n",
      "Processing 466: easyDacha\n",
      "******************************\n",
      "Processing 467: Taylor AI\n",
      "******************************\n",
      "Processing 468: Jori\n",
      "******************************\n",
      "Processing 469: Hightime AI\n",
      "******************************\n",
      "Processing 470: Q by TENET\n",
      "******************************\n",
      "Processing 471: PomPom Fairy\n",
      "******************************\n",
      "Processing 472: Eyemote Vision Inc.\n",
      "******************************\n",
      "Processing 473: Dataplatr\n",
      "******************************\n",
      "Processing 474: Titan Flood\n",
      "******************************\n",
      "Processing 475: TimeTick\n",
      "******************************\n",
      "Processing 476: Dimension AI\n",
      "******************************\n",
      "Processing 477: CourseFactory\n",
      "******************************\n",
      "Processing 478: LOMA\n",
      "******************************\n",
      "Processing 479: Atomic\n",
      "******************************\n",
      "Processing 480: Tripsby.AI\n",
      "******************************\n",
      "Processing 481: Mithril\n",
      "******************************\n",
      "Processing 482: Deepshot\n",
      "******************************\n",
      "Processing 483: Lega\n",
      "******************************\n",
      "Processing 484: Dealerverse\n",
      "******************************\n",
      "Processing 485: Fan Pier Labs\n",
      "******************************\n",
      "Processing 486: Composio\n",
      "******************************\n",
      "Processing 487: Bacon AI\n",
      "******************************\n",
      "Processing 488: Judie AI\n",
      "******************************\n",
      "Processing 489: Ambivo\n",
      "******************************\n",
      "Processing 490: BenderXpert\n",
      "******************************\n",
      "Processing 491: Chelle AI\n",
      "******************************\n",
      "Processing 492: Reso\n",
      "******************************\n",
      "Processing 493: Ferona\n",
      "******************************\n",
      "Processing 494: SummarAIze\n",
      "******************************\n",
      "Processing 495: Femmistry\n",
      "******************************\n",
      "Processing 496: OpenEnterprise.AI\n",
      "******************************\n",
      "Processing 497: Ribbn\n",
      "******************************\n",
      "Processing 498: Overlap Capital\n",
      "******************************\n",
      "Processing 499: Parsed\n",
      "******************************\n",
      "Processing 500: CanFY\n",
      "******************************\n",
      "Processing 501: Flowfile\n",
      "******************************\n",
      "Processing 502: {descrb}\n",
      "******************************\n",
      "Processing 503: DearAI\n",
      "******************************\n",
      "Processing 504: Transmission\n",
      "******************************\n",
      "Processing 505: crear.ai\n",
      "******************************\n",
      "Processing 506: Legislaide\n",
      "******************************\n",
      "Processing 507: blujin\n",
      "******************************\n",
      "Processing 508: Smartsales.ai\n",
      "******************************\n",
      "Processing 509: Dentexion\n",
      "******************************\n",
      "Processing 510: Getgud.io\n",
      "******************************\n",
      "Processing 511: PerfectEssayWriterAI\n",
      "******************************\n",
      "Processing 512: Auto Learn\n",
      "******************************\n",
      "Processing 513: ProdigyBuild\n",
      "******************************\n",
      "Processing 514: LifeLink\n",
      "******************************\n",
      "Processing 515: Skin Dossier\n",
      "******************************\n",
      "Processing 516: Structure\n",
      "******************************\n",
      "Processing 517: Forward Labs\n",
      "******************************\n",
      "Processing 518: Quantem.io\n",
      "******************************\n",
      "Processing 519: AIGC Chain\n"
     ]
    }
   ],
   "source": [
    "cols = [\"generated_description\", \"generated_description_conf_interval\", \"generated_description_conf_interval_reasoning\"]\n",
    "args = [[\"company\",\"organization name\"], [\"website\",\"website\"], [\"description\",\"description_all\"]]\n",
    "\n",
    "async def main(df):\n",
    "    df = await prompting_class.iterate(df, generated_description_prompt, args, \"generated_description_llm\")\n",
    "    df.to_csv('../output/df_with_generated_description.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = asyncio.run(main(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "800a070f-834d-4946-89aa-9a4b54691506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(text):\n",
    "    if pd.isnull(text): return {}\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"  \",\" \").replace(\"*\",\"\").replace(\" (two sentences)\",\"\").replace(\"/10\",\"\").replace(\"_\",\"\").replace(\"#\",\"\")\n",
    "    pattern = r\"^.*?\\s*Description:?\\s*(.*)Confidence Interval:\\s*(\\d+)\\s*Reasoning:\\s*(.*)$\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        description = match.group(1).strip()\n",
    "        confidence_interval = match.group(2).strip()\n",
    "        reasoning = match.group(3).strip()\n",
    "        \n",
    "        result = {\n",
    "            \"generated_description\": description,\n",
    "            \"generated_description_conf_interval\": confidence_interval,\n",
    "            \"generated_description_conf_interval_reasoning\": reasoning\n",
    "        }\n",
    "        return result\n",
    "        \n",
    "    else:\n",
    "        print(\"FAILURE TO MATCH\")\n",
    "        return {}\n",
    "        \n",
    "for col in cols:\n",
    "    if col in list(df.columns): df=df.drop(columns=[col])\n",
    "        \n",
    "results_df = pd.DataFrame(list(df.apply(lambda x: extract_data(x[\"generated_description_llm\"]),axis=1)))\n",
    "df = pd.concat([df, results_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d25eccff-81c2-4b04-b3d6-32ac58174374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412a723-93bf-4383-85ea-4ddc678aa946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart if cut in the middle of run!\n",
    "# df = df.merge(prompting_class.results_df, on='organization name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35dfc0a6-6140-4e76-bc93-20221bfc8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"generated_description\"],axis=0)\n",
    "df = df.drop_duplicates(subset=['organization name'], keep='first')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1a0600-612d-4cb0-9cf2-3d58039e55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Processing 0: Pika\n",
      "******************************\n",
      "Processing 1: Contextual AI\n",
      "******************************\n",
      "Processing 2: Sierra\n",
      "******************************\n",
      "Processing 3: Essential AI\n",
      "******************************\n",
      "Processing 4: Liquid AI\n",
      "******************************\n",
      "Processing 5: Lindy\n",
      "******************************\n",
      "Processing 6: LlamaIndex\n",
      "******************************\n",
      "Processing 7: MagicSchool AI\n",
      "******************************\n",
      "Processing 8: Norm AI\n",
      "******************************\n",
      "Processing 9: Patronus AI\n"
     ]
    }
   ],
   "source": [
    "cols = [\"parsed_description\", \"parsed_description_conf_interval\", \"parsed_description_conf_interval_reasoning\", \"Tasks/Jobs\",\"Industry\",\"Customers\"]  \n",
    "args = [[\"company\",\"organization name\"],[\"website\",\"website\"], [\"description\",\"generated_description\"]]\n",
    "\n",
    "async def main(df):\n",
    "    df = await prompting_class.iterate(df, parsed_description_prompt, args, \"parsed_description_llm\")\n",
    "    df.to_csv('../output/df_with_parsed_description.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = asyncio.run(main(df[:10]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ac4998d-c73c-49c9-90b8-1dcd80432296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(text):\n",
    "    if pd.isnull(text): return {}\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    text = text.replace(\"_\", \"\").replace(\"*\", \"\").replace(\"#\", \"\")\n",
    "    text = re.sub(r'\\s?\\([^)]*\\)', '', text)\n",
    "    pattern = r\".*?Tasks/Jobs:\\s*(.*?)\\s*Industry:\\s*(.*?)\\s*Customers:\\s*(.*?)\\s*Confidence Interval:\\s*(.*?)\\s*Reasoning:\\s*(.*)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        tasks_jobs = match.group(1).strip()\n",
    "        industry = match.group(2).strip()\n",
    "        customers = match.group(3).strip()\n",
    "        confidence_interval = match.group(4).strip()\n",
    "        reasoning = match.group(5).strip()\n",
    "        \n",
    "        result = {\n",
    "            \"parsed_description\": \n",
    "            \"Tasks/Jobs: \" + tasks_jobs + \"\\n\" + \"Industry: \" + industry + \"\\n\" + \"Customers: \"+ customers,\n",
    "            \"Tasks/Jobs\": tasks_jobs,\n",
    "            \"Industry\": industry,\n",
    "            \"Customers\": customers,\n",
    "            \"parsed_description_conf_interval\": confidence_interval,\n",
    "            \"parsed_description_conf_interval_reasoning\": reasoning\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        print(text)\n",
    "        print(\"FAILURE TO MATCH\")\n",
    "        return {}\n",
    "        \n",
    "for col in cols:\n",
    "    if col in list(df.columns): df=df.drop(columns=[col])\n",
    "        \n",
    "results_df = pd.DataFrame(list(df.apply(lambda x: extract_data(x[\"parsed_description_llm\"]),axis=1)))\n",
    "df = pd.concat([df, results_df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607d5fcb-f1b8-40b5-a33c-e930c074d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"parsed_description\"],axis=0)\n",
    "df = df.drop_duplicates(subset=['organization name'], keep='first')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ccf4a3c-0c10-46bf-9569-c7dbd9d87cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/df_with_parsed_description.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c45e31-1bda-49cc-9833-2b363313f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Processing 0: Pika\n",
      "******************************\n",
      "Processing 1: Contextual AI\n",
      "******************************\n",
      "Processing 2: Sierra\n",
      "******************************\n",
      "Processing 3: Essential AI\n",
      "******************************\n",
      "Processing 4: Liquid AI\n",
      "******************************\n",
      "Processing 5: Lindy\n",
      "******************************\n",
      "Processing 6: LlamaIndex\n",
      "******************************\n",
      "Processing 7: MagicSchool AI\n",
      "******************************\n",
      "Processing 8: Norm AI\n",
      "******************************\n",
      "Processing 9: Patronus AI\n"
     ]
    }
   ],
   "source": [
    "cols = [\"situation1\", \"situation1_conf_interval\", \"situation1_conf_interval_reasoning\", \"situation2\", \"situation2_conf_interval\", \"situation2_conf_interval_reasoning\",\"situation3\", \"situation3_conf_interval\", \"situation3_conf_interval_reasoning\",'Example1','Job1','Job1_title','Example2','Job2','Job2_title','Example3','Job3','Job3_title']\n",
    "args = [[\"company\",\"organization name\"],[\"website\",\"website\"], [\"description\",\"generated_description\"], [\"parsed_description\",\"parsed_description\"]]\n",
    "\n",
    "\n",
    "async def main(df):\n",
    "    df = await prompting_class.iterate(df, examples_prompt, args, \"examples_llm\",1060)\n",
    "    df.to_csv('../output/df_with_examples.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = asyncio.run(main(df[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a5f898-9923-4b08-8f5a-071a280cbefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>organization name</th>\n",
       "      <th>full_description</th>\n",
       "      <th>industries</th>\n",
       "      <th>headquarters location</th>\n",
       "      <th>founded date</th>\n",
       "      <th>description</th>\n",
       "      <th>CB rank</th>\n",
       "      <th>headquarters region</th>\n",
       "      <th>postal code</th>\n",
       "      <th>...</th>\n",
       "      <th>generated_description_conf_interval</th>\n",
       "      <th>generated_description_conf_interval_reasoning</th>\n",
       "      <th>parsed_description_llm</th>\n",
       "      <th>parsed_description</th>\n",
       "      <th>Tasks/Jobs</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Customers</th>\n",
       "      <th>parsed_description_conf_interval</th>\n",
       "      <th>parsed_description_conf_interval_reasoning</th>\n",
       "      <th>examples_llm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Pika</td>\n",
       "      <td>Pika is a startup that is developing an AI-pow...</td>\n",
       "      <td>Artificial Intelligence (AI), Generative AI, G...</td>\n",
       "      <td>Palo Alto, California, United States</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Pika is a startup that develops an AI-powered ...</td>\n",
       "      <td>71</td>\n",
       "      <td>San Francisco Bay Area, Silicon Valley, West C...</td>\n",
       "      <td>94301</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>While the company is in its early stages, info...</td>\n",
       "      <td>Tasks/Jobs: Video creation, Video editing, Scr...</td>\n",
       "      <td>Tasks/Jobs: Video creation, Video editing, Scr...</td>\n",
       "      <td>Video creation, Video editing, Scriptwriting, ...</td>\n",
       "      <td>Video production</td>\n",
       "      <td>Filmmakers, Content creators, Marketers</td>\n",
       "      <td>9</td>\n",
       "      <td>The website and description clearly indicate P...</td>\n",
       "      <td>Here are three examples of how Pika could be u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Contextual AI</td>\n",
       "      <td>Contextual AI offers a pioneering approach to ...</td>\n",
       "      <td>Artificial Intelligence (AI), Generative AI, S...</td>\n",
       "      <td>Mountain View, California, United States</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Contextual AI offers a pioneering approach to ...</td>\n",
       "      <td>217</td>\n",
       "      <td>San Francisco Bay Area, Silicon Valley, West C...</td>\n",
       "      <td>94040</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>The provided description is very general, and ...</td>\n",
       "      <td>Company: Contextual AI\\nWesbite: contextual.ai...</td>\n",
       "      <td>Tasks/Jobs: Content creation, Data analysis, C...</td>\n",
       "      <td>Content creation, Data analysis, Customer serv...</td>\n",
       "      <td>Enterprise software</td>\n",
       "      <td>Businesses, Enterprises</td>\n",
       "      <td>8</td>\n",
       "      <td>The website explicitly states that the company...</td>\n",
       "      <td>Here are three examples of how Contextual AI m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sierra</td>\n",
       "      <td>Sierra is an AI startup that tackles essential...</td>\n",
       "      <td>Artificial Intelligence (AI), Enterprise Softw...</td>\n",
       "      <td>San Francisco, California, United States</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Sierra is an AI startup that tackles essential...</td>\n",
       "      <td>273</td>\n",
       "      <td>San Francisco Bay Area, West Coast, Western US</td>\n",
       "      <td>‚Äî</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>The provided description is a good starting po...</td>\n",
       "      <td>Tasks/Jobs: Customer service, Lead generation,...</td>\n",
       "      <td>Tasks/Jobs: Customer service, Lead generation,...</td>\n",
       "      <td>Customer service, Lead generation, Sales, Tech...</td>\n",
       "      <td>Customer service automation</td>\n",
       "      <td>Businesses, Enterprises</td>\n",
       "      <td>9</td>\n",
       "      <td>The website and description explicitly state t...</td>\n",
       "      <td>Sierra Examples:\\n\\nExample 1: A customer serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Essential AI</td>\n",
       "      <td>Essential AI creates AI solutions that enhance...</td>\n",
       "      <td>Artificial Intelligence (AI), Information Tech...</td>\n",
       "      <td>San Francisco, California, United States</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Essential AI creates AI solutions that enhance...</td>\n",
       "      <td>287</td>\n",
       "      <td>San Francisco Bay Area, West Coast, Western US</td>\n",
       "      <td>‚Äî</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>The provided description from Essential AI's w...</td>\n",
       "      <td>Tasks/Jobs: Text generation, Data analysis, Cu...</td>\n",
       "      <td>Tasks/Jobs: Text generation, Data analysis, Cu...</td>\n",
       "      <td>Text generation, Data analysis, Customer servi...</td>\n",
       "      <td>Business Automation</td>\n",
       "      <td>Enterprises, Businesses, Organizations</td>\n",
       "      <td>8</td>\n",
       "      <td>Essential AI's website and descriptions clearl...</td>\n",
       "      <td>Essential AI Example Uses:\\n\\nExample 1:  A ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Liquid AI</td>\n",
       "      <td>Liquid AI is a developer of AI applications th...</td>\n",
       "      <td>Artificial Intelligence (AI), Generative AI, I...</td>\n",
       "      <td>Cambridge, Massachusetts, United States</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Liquid AI is a developer of AI applications th...</td>\n",
       "      <td>544</td>\n",
       "      <td>Greater Boston Area, East Coast, New England</td>\n",
       "      <td>02142</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>While the provided information is limited, the...</td>\n",
       "      <td>Company: Liquid AI\\nWesbite: liquid.ai\\nDescri...</td>\n",
       "      <td>Tasks/Jobs: Content generation, Idea brainstor...</td>\n",
       "      <td>Content generation, Idea brainstorming, Workfl...</td>\n",
       "      <td>Productivity and Creativity Tools</td>\n",
       "      <td>Content creators, marketers, designers, develo...</td>\n",
       "      <td>8</td>\n",
       "      <td>Liquid AI's website and marketing materials cl...</td>\n",
       "      <td>Here are three examples of how Liquid AI's pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 organization name  \\\n",
       "0           0              Pika   \n",
       "1           1     Contextual AI   \n",
       "2           2            Sierra   \n",
       "3           3      Essential AI   \n",
       "4           4         Liquid AI   \n",
       "\n",
       "                                    full_description  \\\n",
       "0  Pika is a startup that is developing an AI-pow...   \n",
       "1  Contextual AI offers a pioneering approach to ...   \n",
       "2  Sierra is an AI startup that tackles essential...   \n",
       "3  Essential AI creates AI solutions that enhance...   \n",
       "4  Liquid AI is a developer of AI applications th...   \n",
       "\n",
       "                                          industries  \\\n",
       "0  Artificial Intelligence (AI), Generative AI, G...   \n",
       "1  Artificial Intelligence (AI), Generative AI, S...   \n",
       "2  Artificial Intelligence (AI), Enterprise Softw...   \n",
       "3  Artificial Intelligence (AI), Information Tech...   \n",
       "4  Artificial Intelligence (AI), Generative AI, I...   \n",
       "\n",
       "                      headquarters location founded date  \\\n",
       "0      Palo Alto, California, United States   2023-01-01   \n",
       "1  Mountain View, California, United States   2023-01-01   \n",
       "2  San Francisco, California, United States   2023-01-01   \n",
       "3  San Francisco, California, United States   2023-01-01   \n",
       "4   Cambridge, Massachusetts, United States   2023-01-01   \n",
       "\n",
       "                                         description CB rank  \\\n",
       "0  Pika is a startup that develops an AI-powered ...      71   \n",
       "1  Contextual AI offers a pioneering approach to ...     217   \n",
       "2  Sierra is an AI startup that tackles essential...     273   \n",
       "3  Essential AI creates AI solutions that enhance...     287   \n",
       "4  Liquid AI is a developer of AI applications th...     544   \n",
       "\n",
       "                                 headquarters region postal code  ...  \\\n",
       "0  San Francisco Bay Area, Silicon Valley, West C...       94301  ...   \n",
       "1  San Francisco Bay Area, Silicon Valley, West C...       94040  ...   \n",
       "2     San Francisco Bay Area, West Coast, Western US           ‚Äî  ...   \n",
       "3     San Francisco Bay Area, West Coast, Western US           ‚Äî  ...   \n",
       "4       Greater Boston Area, East Coast, New England       02142  ...   \n",
       "\n",
       "  generated_description_conf_interval  \\\n",
       "0                                   7   \n",
       "1                                   7   \n",
       "2                                   7   \n",
       "3                                   7   \n",
       "4                                   7   \n",
       "\n",
       "       generated_description_conf_interval_reasoning  \\\n",
       "0  While the company is in its early stages, info...   \n",
       "1  The provided description is very general, and ...   \n",
       "2  The provided description is a good starting po...   \n",
       "3  The provided description from Essential AI's w...   \n",
       "4  While the provided information is limited, the...   \n",
       "\n",
       "                              parsed_description_llm  \\\n",
       "0  Tasks/Jobs: Video creation, Video editing, Scr...   \n",
       "1  Company: Contextual AI\\nWesbite: contextual.ai...   \n",
       "2  Tasks/Jobs: Customer service, Lead generation,...   \n",
       "3  Tasks/Jobs: Text generation, Data analysis, Cu...   \n",
       "4  Company: Liquid AI\\nWesbite: liquid.ai\\nDescri...   \n",
       "\n",
       "                                  parsed_description  \\\n",
       "0  Tasks/Jobs: Video creation, Video editing, Scr...   \n",
       "1  Tasks/Jobs: Content creation, Data analysis, C...   \n",
       "2  Tasks/Jobs: Customer service, Lead generation,...   \n",
       "3  Tasks/Jobs: Text generation, Data analysis, Cu...   \n",
       "4  Tasks/Jobs: Content generation, Idea brainstor...   \n",
       "\n",
       "                                          Tasks/Jobs  \\\n",
       "0  Video creation, Video editing, Scriptwriting, ...   \n",
       "1  Content creation, Data analysis, Customer serv...   \n",
       "2  Customer service, Lead generation, Sales, Tech...   \n",
       "3  Text generation, Data analysis, Customer servi...   \n",
       "4  Content generation, Idea brainstorming, Workfl...   \n",
       "\n",
       "                            Industry  \\\n",
       "0                   Video production   \n",
       "1                Enterprise software   \n",
       "2        Customer service automation   \n",
       "3                Business Automation   \n",
       "4  Productivity and Creativity Tools   \n",
       "\n",
       "                                           Customers  \\\n",
       "0            Filmmakers, Content creators, Marketers   \n",
       "1                            Businesses, Enterprises   \n",
       "2                            Businesses, Enterprises   \n",
       "3             Enterprises, Businesses, Organizations   \n",
       "4  Content creators, marketers, designers, develo...   \n",
       "\n",
       "  parsed_description_conf_interval  \\\n",
       "0                                9   \n",
       "1                                8   \n",
       "2                                9   \n",
       "3                                8   \n",
       "4                                8   \n",
       "\n",
       "          parsed_description_conf_interval_reasoning  \\\n",
       "0  The website and description clearly indicate P...   \n",
       "1  The website explicitly states that the company...   \n",
       "2  The website and description explicitly state t...   \n",
       "3  Essential AI's website and descriptions clearl...   \n",
       "4  Liquid AI's website and marketing materials cl...   \n",
       "\n",
       "                                        examples_llm  \n",
       "0  Here are three examples of how Pika could be u...  \n",
       "1  Here are three examples of how Contextual AI m...  \n",
       "2  Sierra Examples:\\n\\nExample 1: A customer serv...  \n",
       "3  Essential AI Example Uses:\\n\\nExample 1:  A ma...  \n",
       "4  Here are three examples of how Liquid AI's pro...  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5371762-7636-4272-9569-96dfe2409bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_data(text):\n",
    "    if pd.isnull(text): return {}\n",
    "    # Normalize the text to ensure consistent whitespace and remove unwanted characters.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.replace(\"*\", \"\").replace(\"/10\", \"\").replace(\"_\", \"\").replace(\"#\", \"\")\n",
    "    vals = {}\n",
    "    i = 1\n",
    "    while i <=3:\n",
    "        # Regex pattern adjusted to handle fractions in confidence intervals like '9/10'\n",
    "        pattern = rf\"Example\\s+{i}:\\s+(.*?)\\s+ONET JOB automated\\s+{i}:\\s+(.*?)\\s+ONET JOB\\s+{i}:\\s+(.*?)\\s+Confidence Interval\\s+{i}:\\s+(\\d+(?:/\\d+)?)\\s+Reasoning\\s+{i}:\\s+(.*?)(?=\\s*Example\\s+{i + 1}:|$)\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if not match:\n",
    "            print(text+\"\\n\\n\")\n",
    "            print(f\"No matches found for Example {i}\")  # Debug if no examples are found\n",
    "            return {}\n",
    "            break\n",
    "\n",
    "        example_text, onet_job_automated, onet_job, confidence_interval, reasoning = match.groups()\n",
    "        example_key = f\"Example{i}\"\n",
    "        vals[example_key] = example_text.strip()\n",
    "        vals[f\"Job{i}\"] = onet_job_automated.strip()\n",
    "        vals[f\"Job{i}_title\"] = onet_job.strip()\n",
    "        vals[f\"situation{i}_conf_interval\"] = confidence_interval.strip()\n",
    "        vals[f\"situation{i}_conf_interval_reasoning\"] = reasoning.strip()\n",
    "\n",
    "        i += 1  # Prepare to search for the next example\n",
    "    return vals\n",
    "\n",
    "for col in cols:\n",
    "    if col in list(df.columns): df=df.drop(columns=[col])\n",
    "\n",
    "results_df = pd.DataFrame(list(df.apply(lambda x: extract_data(x[\"examples_llm\"]),axis=1)))\n",
    "df = pd.concat([df, results_df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32d75990-4417-4b85-bece-5b2c7023bd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = pd.read_csv('../output/df_with_examples.csv',index_col=0)\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77683a4c-58da-45d6-a2dd-c8d631a4d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['organization name'], keep='first')\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62096dd8-d9b4-4041-a9e6-0ea4b79670fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71131668-ccba-4d06-b647-593bf6be74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Example1\"],axis=0)\n",
    "df = df.drop_duplicates(subset=['organization name'], keep='first')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98386740-878f-4498-9154-e590491a3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/df_with_examples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c86d0d-ba5d-4e71-8457-1211b89f2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RERUN STOPPED IN MIDDLE\n",
    "# results_df = pd.read_csv(\"result.csv\",index_col=0)\n",
    "# df = pd.read_csv('../output/df_with_parsed_description.csv',index_col=0)\n",
    "# prompting_class.set_current_results_df(results_df)\n",
    "# df = df.merge(results_df, on='organization name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e0fb1-0fbd-4fa9-a401-06e3ed54053f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
